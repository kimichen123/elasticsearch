[[getting-started]]
= 开始

[partintro]
--

Elasticsearch 是一个高度可扩展的开源全文搜索和分析引擎。它具有存储、搜索以及快速近实时分析海量数据的功能，并通常为具有复杂搜索特性和需求的应用提供底层 引擎/计算。
下面是 Elasticsearch 的几个使用用例:

* 你正在经营一家支持客户搜索在售商品的网上商店。在这种场景下，你可以使用 Elasticsearch 存储所有的产品目录和库存，并提供搜索和自动补全的功能。
* 你想收集日志和事务数据并分析和挖掘这些数据，以便于查找趋势、统计信息、汇总信息或者异常情形。在这种场景下，你可以使用 Logstash(Elasticsearch/Logstash/Kibana 栈的一部分) 来收集、聚合和解析数据，然后通过 Logstash 将这些数据存入 Elasticsearch。一旦这些数据存在 Elasticsearch 中，你就可以使用搜索和聚合方法来挖掘你感兴趣的信息。
* 你运转一个为某些精明的客户指定一条类似 “我对购买一个特定的电子产品感兴趣，如果下个月任何供应商的报价低于 $X 则通知我” 的价格提醒平台。在这种场景下，你可以收集供应商价格并将这些价格推送到 Elasticsearch,然后使用反向搜索(过滤)功能来匹配客户查询的价格变动，并在找到满足匹配条件情形下给客户提醒。
* 你有分析/商业智能需求，并希望快速调查、分析、可视化以及基于海量数据（比如数百万或数十亿的记录）的问题咨询。在这种场景下,你可以使用 Elasticsearch 存储你的数据,然后使用 Kibana(Elasticsearch/Logstash/Kibana 栈的一部分)来构建自定义仪表板以呈现对你很重要的可视化数据。此外，你可以使用 Elasticsearch 聚合功能来对数据执行复杂的商业智能查询。

在本教程的剩余部分中将指导你开始使用、运行、深入了解 Elasticsearch，以及执行如索引、搜索、和修改数据等基本操作。在本教程的最后，你应该会很好地了解Elasticsearch是什么，以及它是如何工作的，并希望从中可以收获启发以便利用它来构建复杂的搜索应用程序，或者从你的数据中挖掘情报。
--

== 基本概念

以下是一些关于 Elasticsearch 的核心概念。理解这些概念可以极大的简化学习过程。

[float]
=== 近实时 (NRT)

Elasticsearch 是一个近实时搜索平台。这意味着从你索引文档到其可搜索会有一个轻微的延迟（通常是 1s）。

[float]
=== 集群

集群是一个或多个节点（服务器）的集合，这些节点共同保存所有数据并提供联合索引和搜索能力。一个集群由一个唯一名称标识，默认名称是 "elasticsearch"。这个名称很重要，因为如果节点被设置为按名称加入集群，那么节点就只能是集群的一部分。

在不同的环境中请确保不要使用相同的集群名，否则可能导致节点加入错误的集群。例如，你可以使用 `logging-dev` 、`logging-stage` 、`logging-prod` 来标识开发、阶段、生产集群。

注意只有一个节点的集群是完全合适和合理的。此外，你也可以有多个唯一名称标识的独立集群。

[float]
=== 节点

节点是集群中的具有存储数据以及参与集群索引、搜索功能的单台服务器。和集群类似，一个节点由一个名称标识，默认情况下该名称是在启动时分配的一个随机通用唯一识别码（UUID）。
如果你不想要默认名称则可以自定义。节点名称在管理和确认网络中哪些服务器在 Elasticsearch 集群的作用十分重要。

节点可以配置成按集群名称加入特定集群。默认情况下，每个节点设置成加入名为 `elasticsearch` 的集群，这意味着一旦你启动了网络中的多个节点，且假设这些节点可以相互发现，那么这些节点将自动组成一个名为 `elasticsearch` 的集群。

单个集群中可以有任意多的节点。此外，如果网络中没有其他正在运行的 Elasticsearch 节点，则默认情况下启动单个节点会形成一个名为 `elasticsearch` 的单节点集群。

[float]
=== 索引

索引是一个具有相似特性的文档集合。例如你可以使用三个索引分别存储客户数据、产品目录和订单数据。索引是由名称（必须全部为小写）标识的，同时该名称作为对文档进行索引、搜索、更新和删除操作的引用。

单个集群中你可以定义任意多的索引。

[float]
=== 类型

deprecated[6.0.0,参考 <<removal-of-types>>]

类型曾经是同一索引中允许存储不同类型文档的一个逻辑 类别/分区。例如一个是用户类型，另一个是博客文章类型。 现在索引中再也不能创建多个类型，同时在未来版本中将移除整个类型的概念。更多信息请参考 <<removal-of-types>>。

[float]
=== 文档

文档是可被索引的基本单元。例如你可以使用三条文档分布存储单个客户、单个产品以及单个订单。文档是由互联网中常见的数据交换格式 http://json.org/[JSON] (JavaScript Object Notation) 呈现的。

在索引/类型中可以存储任意多的文档。需要注意的是尽管文档表面上是存储在索引中，但实际上文档必须被 索引/分配 给索引内的类型。

[[getting-started-shards-and-replicas]]
[float]
=== 分片 & 副本

索引可以存储大量数据，这些数据可能会超出单个节点的硬件限制。例如，使用单个节点的磁盘存储占用 1TB 磁盘空间的十亿份文档不大合适，另外单节点在处理搜索请求也可能会比较慢。

为了解决这些问题，Elasticsearch 提供了将索引细分为多个分片的能力。当你创建索引时，你可以定义你需要的分片数，每个分片都是分布在集群中任意节点的一个功能齐全的独立 "index"。

分片如此重要主要有两个原因：

* 它允许水平 划分/缩放 你的内容。
* 它允许你跨分片（可能在多个节点上）进行分发和并行操作以提升 性能/吞吐量。


分片分发和文档搜索时聚合的机制是完全由 Elasticsearch 管理的，且对用户来说是透明的。

在 网络/云 环境下失败可能发生在任何时刻。因此强烈建议当发生 分片/节点 下线或消失时需要有一套故障转移机制。基于此，Elasticsearch 允许你设置一个或者多个索引分片的拷贝，这些拷贝被称为副本分片或简称副本。

复制如此重要主要有两个原因：

* 它在 分片/节点 失败的情况下提供了高可用。因为这个原因，需要特别注意的是副本分片永远不会分配在与 原始/主 分片相同的节点上。
* 它允许你扩展你的 搜索量/吞吐量，因为搜索可在所有副本上执行。


总之，每个索引可以分为多个分片。一个索引可以有 0 个（意味着每一副本）或者多个副本。一旦设置了副本，每个索引都将有主分片（原始分片）和副分片（主分片的拷贝）。分片和副本数在创建索引时定义，创建索引之后，你可以随时动态改变副本数，但是不能随意改变分片数。

默认情况下，Elasticsearch 中的每个索引由 5 个主分片和 1 个副本组成，这意味着你的集群至少需要两个节点，索引将包含 5 个主分片和 5 个副本（1个完整副本），总共10个分片。

NOTE: 每个 Elasticsearch 分片是一个 Lucence 索引。一个 Lucene 索引有最大支持文档数。从 https://issues.apache.org/jira/browse/LUCENE-5843[`LUCENE-5843`] 开始, 最大限制是 `2,147,483,519` (= Integer.MAX_VALUE - 128)。
你可以使用 {ref}/cat-shards.html[`_cat/shards`] API 管理分片数。

有了这些，我们就可以开始有趣之旅了...

== 安装

Elasticsearch 最低需要Java 8。 在撰写本文时强烈建议你使用 Oracle JDK 版本 {jdk}。Java 不同平台的安装方法不同，所以我们不会在这里详细讨论安装细节。Oracle 的建议安装文档可以在 http://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html[Oracle's website] 找到。得知道的是，在安装 Elasticsearch 之前请先检查你的 Java 版本（请按需进行 安装/升级）：

[source,sh]
--------------------------------------------------
java -version
echo $JAVA_HOME
--------------------------------------------------

当设置好了 Java 环境之后，我们可以下载运行 Elasticsearch。所有已发布的二进制版本可以从 http://www.elastic.co/downloads[`www.elastic.co/downloads`] 下载。对于每一个发布版本，你可以从 `zip` 、 `tar` 、 `DEB` 、 `RPM` 或者 Windows `MSI` 中选择合适版本进行安装。

[float]
=== 安装 tar 示例

简单起见，我们使用 {ref}/zip-targz.html[tar] 文件。

下载 Elasticsearc {version} tar：

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}.tar.gz
--------------------------------------------------
// NOTCONSOLE

然后解压：

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
tar -xvf elasticsearch-{version}.tar.gz
--------------------------------------------------

解压后当前目录中将创建一组文件和文件夹。接着进入 bin 目录:

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
cd elasticsearch-{version}/bin
--------------------------------------------------

下面我们准备启动我们的节点和单集群：

[source,sh]
--------------------------------------------------
./elasticsearch
--------------------------------------------------

[float]
=== 使用 Homebrew 安装

在 macOS, Elasticsearch 可以使用 https://brew.sh[Homebrew] 安装：

["source","sh"]
--------------------------------------------------
brew install elasticsearch
--------------------------------------------------

[float]
=== MSI Windows Installer 安装示例

对于 Windows 用户，我们推荐使用 {ref}/windows.html[MSI Installer package] 安装。 这个包包含的图形用户界面（GUI）可以引导你完成安装。

首先，从 https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}.msi 下载 Elasticsearch {version} MSI。

然后双击下载的文件以启动 GUI。在第一个界面中选择部署目录：

[[getting-started-msi-installer-locations]]
image::images/msi_installer/msi_installer_locations.png[]

选择是否安装为服务，或者按需手动启动 Elasticsearch。
为了与 tar 示例保持一致，这里选择不安装服务：

[[getting-started-msi-installer-service]]
image::images/msi_installer/msi_installer_no_service.png[]

只需保留配置的默认值：

[[getting-started-msi-installer-configuration]]
image::images/msi_installer/msi_installer_configuration.png[]

同样为了与 tar 示例保持一致，取消所有插件以避免安装任何插件:

[[getting-started-msi-installer-plugins]]
image::images/msi_installer/msi_installer_plugins.png[]

点击安装按钮后，Elasticsearch 将会开始安装：

[[getting-started-msi-installer-success]]
image::images/msi_installer/msi_installer_success.png[]

默认情况下 Elasticsearch 会安装在 `%PROGRAMFILES%\Elastic\Elasticsearch` 。在这导航并进入 bin 目录：

**命令提示符：**

[source,sh]
--------------------------------------------------
cd %PROGRAMFILES%\Elastic\Elasticsearch\bin
--------------------------------------------------

**PowerShell：**

[source,powershell]
--------------------------------------------------
cd $env:PROGRAMFILES\Elastic\Elasticsearch\bin
--------------------------------------------------

现在启动节点和单集群：

[source,sh]
--------------------------------------------------
.\elasticsearch.exe
--------------------------------------------------

[float]
=== 节点运行成功

如果一切顺利，你会看到如下的一串提示：

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
[2016-09-16T14:17:51,251][INFO ][o.e.n.Node               ] [] initializing ...
[2016-09-16T14:17:51,329][INFO ][o.e.e.NodeEnvironment    ] [6-bjhwl] using [1] data paths, mounts [[/ (/dev/sda1)]], net usable_space [317.7gb], net total_space [453.6gb], spins? [no], types [ext4]
[2016-09-16T14:17:51,330][INFO ][o.e.e.NodeEnvironment    ] [6-bjhwl] heap size [1.9gb], compressed ordinary object pointers [true]
[2016-09-16T14:17:51,333][INFO ][o.e.n.Node               ] [6-bjhwl] node name [6-bjhwl] derived from node ID; set [node.name] to override
[2016-09-16T14:17:51,334][INFO ][o.e.n.Node               ] [6-bjhwl] version[{version}], pid[21261], build[f5daa16/2016-09-16T09:12:24.346Z], OS[Linux/4.4.0-36-generic/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_60/25.60-b23]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [aggs-matrix-stats]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [ingest-common]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [lang-expression]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [lang-mustache]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [lang-painless]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [percolator]
[2016-09-16T14:17:51,968][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [reindex]
[2016-09-16T14:17:51,968][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [transport-netty3]
[2016-09-16T14:17:51,968][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [transport-netty4]
[2016-09-16T14:17:51,968][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded plugin [mapper-murmur3]
[2016-09-16T14:17:53,521][INFO ][o.e.n.Node               ] [6-bjhwl] initialized
[2016-09-16T14:17:53,521][INFO ][o.e.n.Node               ] [6-bjhwl] starting ...
[2016-09-16T14:17:53,671][INFO ][o.e.t.TransportService   ] [6-bjhwl] publish_address {192.168.8.112:9300}, bound_addresses {{192.168.8.112:9300}
[2016-09-16T14:17:53,676][WARN ][o.e.b.BootstrapCheck     ] [6-bjhwl] max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]
[2016-09-16T14:17:56,718][INFO ][o.e.c.s.ClusterService   ] [6-bjhwl] new_master {6-bjhwl}{6-bjhwl4TkajjoD2oEipnQ}{8m3SNKoFR6yQl1I0JUfPig}{192.168.8.112}{192.168.8.112:9300}, reason: zen-disco-elected-as-master ([0] nodes joined)
[2016-09-16T14:17:56,731][INFO ][o.e.h.HttpServer         ] [6-bjhwl] publish_address {192.168.8.112:9200}, bound_addresses {[::1]:9200}, {192.168.8.112:9200}
[2016-09-16T14:17:56,732][INFO ][o.e.g.GatewayService     ] [6-bjhwl] recovered [0] indices into cluster_state
[2016-09-16T14:17:56,748][INFO ][o.e.n.Node               ] [6-bjhwl] started
--------------------------------------------------

在忽略细节的前提下，我们可以看到我们的节点名为 "6-bjhwl"（在你的场景下会看到不同的字符标识）在单集群中已经被选为主节点。 现在不用太关心主节点，最重要的是我们已经在集群中启动了一个节点。

如上所述，我们可以重命名集群或节点名。可以在启动 Elasticsearch 时使用命令行：

[source,sh]
--------------------------------------------------
./elasticsearch -Ecluster.name=my_cluster_name -Enode.name=my_node_name
--------------------------------------------------

还需注意的是标记为 http 的行提供了节点可访问的 HTTP 地址（ `192.168.8.112` ）和端口（ `9200` ）信息。默认情况下，Elasticsearch 使用端口 `9200` 来提供 REST API 访问。如果需要，这个端口是可配置的。

== 探索你的集群

[float]
=== REST API

现在我们的节点（集群）正在运行，下一步就是需要知道如何与其交互。幸运的是，Elasticsearch 提供了一个全面而强大的 REST API 使你可以与其交互。这些 API 可以完成如下事情：

* 检查集群、节点、索引健康、索引状态和统计信息
* 管理集群、节点、索引数据和元数据
* 对你的索引执行 CRUD （创建、读取、更新、删除）和搜索操作
* 执类似分页、排序、过滤、脚本、聚合和其他高级操作

=== 集群健康

我们从一个基础健康校验开始来看看集群是如果工作的。我们将使用 curl 来演示，你也可以使用任何可以执行 HTTP/REST 调用的工具。假设我们处在 Elasticsearch 集群的节点上，接下来打打开一个新的命令执行窗口。

为了检查集群健康，我们可以调用 {ref}/cat.html[`_cat` API]。你可以在 {kibana-ref}/console-kibana.html[Kibana's Console] 运行 "VIEW IN CONSOLE" ，或者点击下面的 "COPY AS CURL" 链接然后复制到终端执行 `curl` 。

[source,js]
--------------------------------------------------
GET /_cat/health?v
--------------------------------------------------
// CONSOLE

响应如下：

[source,txt]
--------------------------------------------------
epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1475247709 17:01:49  elasticsearch green           1         1      0   0    0    0        0             0                  -                100.0%
--------------------------------------------------
// TESTRESPONSE[s/1475247709 17:01:49  elasticsearch/\\d+ \\d+:\\d+:\\d+ docs_integTestCluster/]
// TESTRESPONSE[s/0             0                  -/0             \\d+                  -/]
// TESTRESPONSE[_cat]

可以看到名为 "elasticsearch" 的集群状态是绿色的。

任何时候查看集群健康，我们只会得到绿色、黄色和红色三种状态之一。

    * 绿色 - 一切正常（集群所有功能可用）
    * 黄色 - 索引数据是可用的但是某些副本尚未分配（集群所有功能可用）
    * 红色 - 索引数据因为某些原因不可用（集群部分功能可用）

**Note:** 当集群状态是红色时，来自可用分片的搜索请求仍然可用，但是因为有未分配的分片所以你需要尽快修复。

上面的响应因为还没有数据所以我们可以看到总共有 1 个节点和 0 个分片。请注意由于我们使用的是默认集群名 （elasticsearch），同时 Elasticsearch 默认使用单播发现来寻找同一机器的其他节点。
无意启动的多个节点时可能都会加入该集群，此时你可能在响应中会看到多个节点。

获取集群中的节点列表：

[source,js]
--------------------------------------------------
GET /_cat/nodes?v
--------------------------------------------------
// CONSOLE

响应如下：

[source,txt]
--------------------------------------------------
ip        heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
127.0.0.1           10           5   5    4.46                        mdi      *      PB2SGZY
--------------------------------------------------
// TESTRESPONSE[s/10           5   5    4.46/\\d+ \\d+ \\d+ (\\d+\\.\\d+)? (\\d+\\.\\d+)? (\\d+\.\\d+)?/]
// TESTRESPONSE[s/[*]/[*]/ s/PB2SGZY/.+/ _cat]

可以看到当前集群只有一个名为 "PB2SGZY" 的单节点。

=== 列举分片

现在来看一下索引情况：

[source,js]
--------------------------------------------------
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE

响应如下：

[source,txt]
--------------------------------------------------
health status index uuid pri rep docs.count docs.deleted store.size pri.store.size
--------------------------------------------------
// TESTRESPONSE[_cat]

这意味着当前集群没有索引。

=== 创建索引

现在创建一个名为 "customer" 的索引，并重新执行列举索引操作：

[source,js]
--------------------------------------------------
PUT /customer?pretty
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE

第一个命令使用 PUT 创建了一个名为 "customer" 的索引，我们可以在调用尾端追加 `pretty` 以获得完美排版的 JSON 响应格式。

响应如下：

[source,txt]
--------------------------------------------------
health status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   customer 95SQ4TSUT7mWBT7VNHH67A   5   1          0            0       260b           260b
--------------------------------------------------
// TESTRESPONSE[s/95SQ4TSUT7mWBT7VNHH67A/.+/ s/260b/\\d+\\.?\\d?k?b/ _cat]

从第二个命令返回的结果可知有 1 个名为 customer 的索引，该索引有 5 个主分片和 1 个副本（默认），同时没有任何文档。

注意到 customer 索引的健康状态为黄色。回想一下之前的讨论，黄色意味着有副本尚未分配。原因是 Elasticsearch 的索引默认有一个副本。因为当前只有一个节点在运行所以索引副本无法被分配（为了高可用），之后有其他节点加入集群时副本才会分配，此时索引的监控状态会变成绿色。

=== 索引和查找文档


现在将一些内容存入 customer 索引。首先添加一个 ID 为 1 的简单 customer 文档。

[source,js]
--------------------------------------------------
PUT /customer/doc/1?pretty
{
  "name": "John Doe"
}
--------------------------------------------------
// CONSOLE

响应如下：

[source,js]
--------------------------------------------------
{
  "_index" : "customer",
  "_type" : "doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
--------------------------------------------------
// TESTRESPONSE[s/"_seq_no" : 0/"_seq_no" : $body._seq_no/ s/"_primary_term" : 1/"_primary_term" : $body._primary_term/]

通过上文可知，可以发现有一个新的 customer 文档成功的被创建。这条文档在索引时指定其内部 id 为 1。

请注意 Elasticsearch 不需要你在索引文档前先显式的创建索引。在上面的例子中，如果 customer 索引不存在Elasticsearch 将自动创建一个 customer 索引。

现在取回刚刚索引的文档：

[source,js]
--------------------------------------------------
GET /customer/doc/1?pretty
--------------------------------------------------
// CONSOLE
// TEST[continued]

响应如下：

[source,js]
--------------------------------------------------
{
  "_index" : "customer",
  "_type" : "doc",
  "_id" : "1",
  "_version" : 1,
  "found" : true,
  "_source" : { "name": "John Doe" }
}
--------------------------------------------------
// TESTRESPONSE

不同的是，响应中会有 `found` 字段，该字段说明找到了 ID 为 1 的文档，另外还有 `_source` 字段，这个字段返回了之前我们索引文档的 JSON 内容。

=== 删除索引

现在执行删除索引操作，并列举出所有索引：

[source,js]
--------------------------------------------------
DELETE /customer?pretty
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE
// TEST[continued]

响应如下：

[source,txt]
--------------------------------------------------
health status index uuid pri rep docs.count docs.deleted store.size pri.store.size
--------------------------------------------------
// TESTRESPONSE[_cat]

意味着索引已经成功删除，现在回到了创建集群时的状态。

在继续学习之前，让我们回头看看迄今为止学到的一些 API 命令：

[source,js]
--------------------------------------------------
PUT /customer
PUT /customer/doc/1
{
  "name": "John Doe"
}
GET /customer/doc/1
DELETE /customer
--------------------------------------------------
// CONSOLE

如果我们仔细研究上面的命令，我们可以发现访问 Elasticsearch 数据的模式。这个模式总结如下：

[source,js]
--------------------------------------------------
<REST Verb> /<Index>/<Type>/<ID>
--------------------------------------------------
// NOTCONSOLE

这个 REST 访问模式在所有的 API 命令中使用十分普遍，简单记住它是掌握 Elasticsearch 的一个好开端。

== 修改数据

Elasticsearch 支持近实时的数据操作和搜索功能。默认情况下，在执行 索引/更新/删除 数据操作后到搜索结果可见大概有一秒的延迟（刷新间隔）。这是与类似 SQL 等其他平台的重要区别，这些平台数据在事务完成后是立即可见的。

[float]
=== 索引/替换 文档

回忆一下之前索引文档的命令：

[source,js]
--------------------------------------------------
PUT /customer/doc/1?pretty
{
  "name": "John Doe"
}
--------------------------------------------------
// CONSOLE


上文将 ID 为 1 的文档索引到 customer 索引，如果我们用另一个新文档执行以上命令，Elasticsearch 会将 ID 为 1 的替换为该新文档：

[source,js]
--------------------------------------------------
PUT /customer/doc/1?pretty
{
  "name": "Jane Doe"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

上文将 ID 为 1 的文档从 "John Doe" 替换为 "Jane Doe"。另一方面，如果我们使用另一个 ID，新文档将会被索引，索引中已经存在的文档将保持不变。

[source,js]
--------------------------------------------------
PUT /customer/doc/2?pretty
{
  "name": "Jane Doe"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

上文索引了一个 ID 为 2 的新文档。

当索引文档时 ID 是可选的。如果没有指定，Elasticsearch 会为文档生成一个随机 ID。ELasticsearch 生成的实际 ID（包括上面提显式指定的 ID） 将作为索引 API 调用的一部分而返回。

下面这个例子展示了索引文档时不显式指定 ID：

[source,js]
--------------------------------------------------
POST /customer/doc?pretty
{
  "name": "Jane Doe"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

注意上例中因为我们没有指定 ID 所有我们使用 `POST` 代替 PUT。

=== 更新文档

除了索引和替换文档之外，我们还可以更新文档。 需要注意的是 Elasticsearch 实际上并没有对原文档进行更新。 无论何时我们进行更新操作，Elasticsearch 都会删除旧文档，然后索引一个新文档。

下面这个例子演示了将之前的文档（ID 为 1） name 字段更新为  "Jane Doe"：

[source,js]
--------------------------------------------------
POST /customer/doc/1/_update?pretty
{
  "doc": { "name": "Jane Doe" }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

下面这个例子演示了将之前的文档（ID 为 1） name 字段更新为  "Jane Doe" 的同时新增了一个 age 字段：

[source,js]
--------------------------------------------------
POST /customer/doc/1/_update?pretty
{
  "doc": { "name": "Jane Doe", "age": 20 }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

可以使用 scripts 执行更新操作。下例使用 script 将 age 增加 5：

[source,js]
--------------------------------------------------
POST /customer/doc/1/_update?pretty
{
  "script" : "ctx._source.age += 5"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

上面例子中， `ctx._source` 指的是将被替换的当前源文档。

Elasticsearch 提供了在查询时更新多条文档的能力（类似 `SQL UPDATE-WHERE`）。详见 {ref}/docs-update-by-query.html[`docs-update-by-query` API]

=== 删除文档

删除一条文档十分简单。下例演示了如果删除 ID 为 2 的 customer。

[source,js]
--------------------------------------------------
DELETE /customer/doc/2?pretty
--------------------------------------------------
// CONSOLE
// TEST[continued]

参考 {ref}/docs-delete-by-query.html[`_delete_by_query` API] 删除特定查询的所有匹配文档。
需要注意的是删除整个索引比 Delete By Query API 高效的多。

=== 批处理

除了索引、更新和删除文档外，Elasticsearch 还提供了 {ref}/docs-bulk.html[`_bulk` API] 操作执行上述任何操作的能力。这个功能十分重要，因为它在执行多个操作时提供了一个非常有效的机制，并尽可能减少网络往返。

作为一个快速示例，下面的 bulk 操作同时索引了两条文档 （ID 1 - John Doe 和 ID 2 - Jane Doe）。

[source,js]
--------------------------------------------------
POST /customer/doc/_bulk?pretty
{"index":{"_id":"1"}}
{"name": "John Doe" }
{"index":{"_id":"2"}}
{"name": "Jane Doe" }
--------------------------------------------------
// CONSOLE

下例使用 bulk 操作更新了第一条文档（ID 为 1）并删除了第二条文档（ID 为 2）。
This example updates the first document (ID of 1) and then deletes the second document (ID of 2) in one bulk operation:

[source,sh]
--------------------------------------------------
POST /customer/doc/_bulk?pretty
{"update":{"_id":"1"}}
{"doc": { "name": "John Doe becomes Jane Doe" } }
{"delete":{"_id":"2"}}
--------------------------------------------------
// CONSOLE
// TEST[continued]

注意到上面的删除行为，因为删除只需要文档 ID 所以这里不需要相应的源文档。

Bulk API 不会因为一个操作失败而失败。如果一个动作因为某种原因失败了，它将继续执行剩下的动作。 Bulk API返回时将为每个操作提供一个状态（与发送的顺序相同），以便检查特定操作是否失败。

== 探究你的数据

[float]
=== 样本数据集

现在我们已经掌握了一些基本知识，让我们尝试一些更现实的数据集。 我准备了一个关于客户银行账户信息的虚构的 JSON 文档样本。 每条文档结构如下：

[source,js]
--------------------------------------------------
{
    "account_number": 0,
    "balance": 16623,
    "firstname": "Bradshaw",
    "lastname": "Mckenzie",
    "age": 29,
    "gender": "F",
    "address": "244 Columbus Place",
    "employer": "Euron",
    "email": "bradshawmckenzie@euron.com",
    "city": "Hobucken",
    "state": "CO"
}
--------------------------------------------------
// NOTCONSOLE

为了数据多样，这些数据都是使用 http://www.json-generator.com/[`www.json-generator.com/`] 生成的，所以请忽略数据实际值和语义，因为这些数据都是随机产生的。

[float]
=== 加载数据样本集

你可以从 https://github.com/elastic/elasticsearch/blob/master/docs/src/test/resources/accounts.json?raw=true[here] 下载数据集（accounts.json）。解压到我们当前的目录下，并使用如下方法将其加载到集群中：

[source,sh]
--------------------------------------------------
curl -H "Content-Type: application/json" -XPOST 'localhost:9200/bank/account/_bulk?pretty&refresh' --data-binary "@accounts.json"
curl 'localhost:9200/_cat/indices?v'
--------------------------------------------------
// NOTCONSOLE

////
This replicates the above in a document-testing friendly way but isn't visible
in the docs:

[source,js]
--------------------------------------------------
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE
// TEST[setup:bank]
////

响应如下：

[source,txt]
--------------------------------------------------
health status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   bank  l7sSYV2cQXmu6_4rJWVIww   5   1       1000            0    128.6kb        128.6kb
--------------------------------------------------
// TESTRESPONSE[s/128.6kb/\\d+(\\.\\d+)?[mk]?b/]
// TESTRESPONSE[s/l7sSYV2cQXmu6_4rJWVIww/.+/ _cat]

这意味着我们已经成功的将 1000 条文档批量索引到 bank 索引中（类型为 account ）。

=== 搜索 API

现在我们可以开始一些简单的搜索。有两种执行搜索操作基本方法：一种是通过 {ref}/search-uri-request.html[REST request URI]  发送搜索参数，另外一种是通过 {ref}/search-request-body.html[REST request body] 发送搜索请求。
请求体方法更具表现性，并以更易读的 JSON 格式定义搜索。我们将尝试一个请求 URI 的例子，但在本教程的剩余部分中，我们将专门使用请求体方法。

用于搜索的 REST API 可用 `_search` 访问。下例返回了 bank 索引的所有文档：

[source,js]
--------------------------------------------------
GET /bank/_search?q=*&sort=account_number:asc&pretty
--------------------------------------------------
// CONSOLE
// TEST[continued]

首先解析搜索调用，我们在 bank 索引中进行搜索（ `_search` ），同时 `q=*` 参数表示匹配索引中的所有文档。`sort=account_number:asc` 参数表示对所有返回的文档结果按 account_number 升序排序。`pretty` 参数告诉 Elasticsearch 返回完美排版的 JSON 结果集。

响应如下（部分展示）:

[source,js]
--------------------------------------------------
{
  "took" : 63,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1000,
    "max_score" : null,
    "hits" : [ {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "0",
      "sort": [0],
      "_score" : null,
      "_source" : {"account_number":0,"balance":16623,"firstname":"Bradshaw","lastname":"Mckenzie","age":29,"gender":"F","address":"244 Columbus Place","employer":"Euron","email":"bradshawmckenzie@euron.com","city":"Hobucken","state":"CO"}
    }, {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "1",
      "sort": [1],
      "_score" : null,
      "_source" : {"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"}
    }, ...
    ]
  }
}
--------------------------------------------------
// TESTRESPONSE[s/"took" : 63/"took" : $body.took/]
// TESTRESPONSE[s/\.\.\./$body.hits.hits.2, $body.hits.hits.3, $body.hits.hits.4, $body.hits.hits.5, $body.hits.hits.6, $body.hits.hits.7, $body.hits.hits.8, $body.hits.hits.9/]

从响应结果可以看到到如下部分：

* `took` – Elasticsearch 执行搜索的时间，单位为毫秒
* `timed_out` – 是否超时
* `_shards` – 搜索了多少分片，以及搜索 成功/失败 的分片数
* `hits` – 搜索结果集
* `hits.total` – 符合搜索条件的文件总数
* `hits.hits` – 实际搜索结果数组（默认前 10 条）
* `hits.sort` - 结果排序的字段（没有指定则以 score 排序）
* `hits._score` 和 `max_score` - 现在可以忽略这些字段

下面是使用请求体的搜索替代方法：

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "sort": [
    { "account_number": "asc" }
  ]
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

最大的不同是在 URI 中不传递 `q=*` ，我们为 `_search` API POST 一个 JSON-style 查询请求体。具体的 JSON 查询将在下一部分讨论.

////
Hidden response just so we can assert that it is indeed the same but don't have
to clutter the docs with it:

[source,js]
--------------------------------------------------
{
  "took" : 63,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1000,
    "max_score": null,
    "hits" : [ {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "0",
      "sort": [0],
      "_score": null,
      "_source" : {"account_number":0,"balance":16623,"firstname":"Bradshaw","lastname":"Mckenzie","age":29,"gender":"F","address":"244 Columbus Place","employer":"Euron","email":"bradshawmckenzie@euron.com","city":"Hobucken","state":"CO"}
    }, {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "1",
      "sort": [1],
      "_score": null,
      "_source" : {"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"}
    }, ...
    ]
  }
}
--------------------------------------------------
// TESTRESPONSE[s/"took" : 63/"took" : $body.took/]
// TESTRESPONSE[s/\.\.\./$body.hits.hits.2, $body.hits.hits.3, $body.hits.hits.4, $body.hits.hits.5, $body.hits.hits.6, $body.hits.hits.7, $body.hits.hits.8, $body.hits.hits.9/]

////

需要明白的是，一旦你得到了搜索结果，ELasticsearch 则完成了这次请求并不会维护任何服务端资源或者为结果集开一个游标。这与类似 SQL 的其他平台形成了鲜明对比，其他平台可能会先获得查询的部分结果集，在你还需要获取（翻阅）剩下的结果时使用某种有状态的服务端游标。

=== 查询语言介绍

Elasticsearch 提供了一种执行查询时的 JSON-style 的特定领域语言。这常被称为 {ref}/query-dsl.html[Query DSL]。这种查询语言十分全面，乍看可能会被吓到，但是实际上最好的方法是先从一些简单的例子开始学习。

回到上一个例子，我们执行了以下查询：

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

解析上述内容，`query` 部分告诉我们查询的定义是什么，`match_all` 是我们想执行的查询类型。`match_all` 在搜索时对指定索引返回所有文档。

除了 `query` 参数，我们可以传递其他参数以影响搜索结果。上面的例子中我们传递了 `sort` ，下面我们传递 `size` 。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "size": 1
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

注意如果 `size` 没有指定，默认是 10。

下例执行了 `match_all` 并返回 11 到 20 的文档结果。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "from": 10,
  "size": 10
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

`from` 参数（从 0 开始）表示索引文档的开始位置，`size` 参数表示从 from 参数开始返回多少文档。这个功能在实现分页功能时十分有用。注意 `from` 没有指定时默认为 0。

下例执行了 `match_all` 并按降序对 balance 进行排序，且返回了前 10（默认大小） 条文档。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "sort": { "balance": { "order": "desc" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

=== 执行搜索

我们已经知道了一些基本搜索参数，现在挖掘一些 Query DSL 的其他内容。先看一眼返回的文档字段。默认情况下，搜索的返回结果中有一个完整的 JSON 文档。这个 JSON 被称为源（搜索匹配中的 `_source` 字段）。如果我们不需要返回整个源文档，我们可以请求源内容中的部分字段。

下例展示了如何从搜索中返回 `account_number` 和 `balance`(`_source` 内部)两个字段：

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "_source": ["account_number", "balance"]
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

需要注意的是上例中只会减少 `_source` 内的字段。它仍然会返回 `_source` 字段 ，但该字段中仅仅包含 `account_number` 和 `balance` 字段。

如果你有 SQL 背景可知上例与 `SQL SELECT FROM` 的概念相似。

现在把注意力转移到 query 部分。之前我们已经见过使用 `match_all` 查询匹配所有文档。现在介绍一种被称为 {ref}/query-dsl-match-query.html[`match` query] 的新查询方法，这个查询可以看做是基本的字段搜索查询（即针对特定字段或字段集进行的搜索）。

下例返回 accout 值为 20 的结果：

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "account_number": 20 } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

下例返回 address 字段中包含 "mill" 词条的所有账户。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "address": "mill" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

下例返回 address 字段中包含 "mill" 或 "lane" 词条的所有账户。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "address": "mill lane" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

下例是 `match` （ `match_phrase` ） 的变体，它返回 address 字段包含 "mill lane"  短语的所有账户。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_phrase": { "address": "mill lane" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

现在介绍 {ref}/query-dsl-bool-query.html[`bool` query]。`bool` 查询允许我们将多个小查询按 boolean 逻辑组合成大查询。

下例组合了两个 `match` 查询并返回 address 字段中包含 "mill" 和 "lane" 词条的所有账户。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

上例中，`bool must` 子句指定匹配的文档必须符合所有查询条件。

相反，下例组合了两个 `match` 查询并返回 address 字段中包含 "mill" 或 "lane" 词条的所有账户。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

上例中，`bool should` 子句指定了一个查询列表，只要满足任意一个查询条件就被视为匹配的文档。

下例组合了两个 `match` 查询并返回 address 字段中既不包含 "mill" 也不包含 "lane" 词条的所有账户。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must_not": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

上例中，`bool must_not` 子句指定了一个查询列表，所有查询条件都不满足才被视为匹配的文档。

我们可以在一个 `bool` 查询中结合 `must` 、 `should` 、 和 `must_not` 子句。此外，我们可以在任意 `bool` 子句中编写 `bool` 查询来模拟任何复杂的多级 boolean 逻辑。

下例返回了年龄为 40 岁且 不住在 ID（aho）的账户。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "age": "40" } }
      ],
      "must_not": [
        { "match": { "state": "ID" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

=== 执行过滤器

上一节中，我们跳过了关于 score （搜索结果的 `_score` 字段）细节。score 是表示文档与指定搜索查询的匹配程度的一个数值。score 越高，文档越相关，score 越低，文档越不相关。

但是查询并不总是需要生成分数，特别是当它们仅仅用于 "filtering" 文档集。Elasticsearch 检测到这种场景时便会自动优化查询执行以便不计算无用的分数。

上一节中介绍的 {ref}/query-dsl-bool-query.html[`bool` query] 同样支持 `filter` 子句，它允许使用 query 来限制将被其他子句匹配的文档，而不改变计算分数的方式。例如 {ref}/query-dsl-range-query.html[`range` query] 允许我们使用范围来过滤文档，这通常用于数字或日期过滤。

本例使用 bool 查询返回余额在 20000 到 30000 的所有账户。换句话说，我们要查找余额大于等于 20000 且小于等于 30000 的所有账户。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": { "match_all": {} },
      "filter": {
        "range": {
          "balance": {
            "gte": 20000,
            "lte": 30000
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

解析上述内容，bool 查询包含一个 `match_all` 查询（查询部分） 和一个 `range` 查询（过滤器部分）。我们可以使用任何其他查询替换为查询和过滤器部分。在这种情况下，range 查询是是十分有意义的因为落入该范围的文档完全 "equally"，即没有哪个文档比其他文档更相关。

除了 `match_all` 、 `match` 、 `bool` 和 `range` 查询，还有很多其他查询类型可用，我们不在这里介绍。由于我们已经对其工作原理有了基本理解，所以将这些知识应用到其他查询进行学习和实验并不难。

=== 执行聚合

聚合提供了数据分组和提取统计数据的能力。理解聚合最简单的方法是将其大致等同于 SQL GROUP BY 和 SQL 聚合功能。在 ELasticsearch 中，你可以在执行搜索并返回匹配结果时在响应中返回与匹配不同的聚合结果。这十分强大和高效的，你可以运行多个查询和多个聚合，并且一次获得所有（两个）操作结果，同时使用简化的 API 来避免网络往返。

首先，下例按状态对所有账户分组，然后返回按降序（默认值）排序的前 10 个（默认）州。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

上面的聚合在 SQL 中类似如下表示：

[source,sh]
--------------------------------------------------
SELECT state, COUNT(*) FROM bank GROUP BY state ORDER BY COUNT(*) DESC
--------------------------------------------------

响应如下（部分展示）：

[source,js]
--------------------------------------------------
{
  "took": 29,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped" : 0,
    "failed": 0
  },
  "hits" : {
    "total" : 1000,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "group_by_state" : {
      "doc_count_error_upper_bound": 20,
      "sum_other_doc_count": 770,
      "buckets" : [ {
        "key" : "ID",
        "doc_count" : 27
      }, {
        "key" : "TX",
        "doc_count" : 27
      }, {
        "key" : "AL",
        "doc_count" : 25
      }, {
        "key" : "MD",
        "doc_count" : 25
      }, {
        "key" : "TN",
        "doc_count" : 23
      }, {
        "key" : "MA",
        "doc_count" : 21
      }, {
        "key" : "NC",
        "doc_count" : 21
      }, {
        "key" : "ND",
        "doc_count" : 21
      }, {
        "key" : "ME",
        "doc_count" : 20
      }, {
        "key" : "MO",
        "doc_count" : 20
      } ]
    }
  }
}
--------------------------------------------------
// TESTRESPONSE[s/"took": 29/"took": $body.took/]

我们可以看到有 27 个账户 住在 `ID` （Idaho） ，接下来是 `TX` （Texas）的 27 个账户，接下来是 `AL` （Alabama）的 25 个账户，等等。

注意到我们设置 `size=0` 不展示搜索结果，因为我们只想在响应中看聚合结果。

在上面的聚合基础上，下例按州（按数量降序排前 10 的状态）计算平均账户余额。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

请注意我们是如何嵌套 `group_by_state` 聚合内的 `average_balance` 聚合。这是聚合的一个通用模式，你可以在任意聚合内嵌套聚合以便从数据中提取所需汇总信息。

在上面的聚合基础上，现在我们按降序对平均 balance 进行排序。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword",
        "order": {
          "average_balance": "desc"
        }
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

下例演示了如果按年龄段（20-29 岁，30-39 岁和 40-49 岁）进行分组，然后再按性别分组，最后得到每个年龄段中每个性别的平均账户 balance。

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_age": {
      "range": {
        "field": "age",
        "ranges": [
          {
            "from": 20,
            "to": 30
          },
          {
            "from": 30,
            "to": 40
          },
          {
            "from": 40,
            "to": 50
          }
        ]
      },
      "aggs": {
        "group_by_gender": {
          "terms": {
            "field": "gender.keyword"
          },
          "aggs": {
            "average_balance": {
              "avg": {
                "field": "balance"
              }
            }
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

我们不会在这里详细介绍其他聚合功能。如果想进行进一步的尝试聚合可以参考 {ref}/search-aggregations.html[aggregations reference guide]。

== 结论

Elasticsearch 是一个即简单又复杂的产品。到目前为止我们已经学习了一些基础知识、内部机制和一些 REST API 的使用。希望本教程能够让你加深对 Elasticsearch 的理解，更重要的是启发你进一步尝试其他重要功能。
