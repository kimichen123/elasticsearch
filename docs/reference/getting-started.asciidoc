[[getting-started]]
= 开始

[partintro]
--

Elasticsearch 是一个高度可扩展的开源全文搜索和分析引擎。它具有存储、搜索以及快速近实时分析海量数据的功能，并通常为具有复杂搜索特性和需求的应用提供底层 引擎/计算。
下面是 Elasticsearch 的几个使用用例:

* 你正在经营一家支持客户搜索在售商品的网上商店。在这种场景下，你可以使用 Elasticsearch 存储所有的产品目录和库存，并提供搜索和自动补全的功能。
* 你想收集日志和事务数据并分析和挖掘这些数据，以便于查找趋势、统计信息、汇总信息或者异常情形。在这种场景下，你可以使用 Logstash(Elasticsearch/Logstash/Kibana 栈的一部分) 来收集、聚合和解析数据，然后通过 Logstash 将这些数据存入 Elasticsearch。一旦这些数据存在 Elasticsearch 中，你就可以使用搜索和聚合方法来挖掘你感兴趣的信息。
* 你运转一个为某些精明的客户指定一条类似 “我对购买一个特定的电子产品感兴趣，如果下个月任何供应商的报价低于 $X 则通知我” 的价格提醒平台。在这种场景下，你可以收集供应商价格并将这些价格推送到 Elasticsearch,然后使用反向搜索(过滤)功能来匹配客户查询的价格变动，并在找到满足匹配条件情形下给客户提醒。
* 你有分析/商业智能需求，并希望快速调查、分析、可视化以及基于海量数据（比如数百万或数十亿的记录）的问题咨询。在这种场景下,你可以使用 Elasticsearch 存储你的数据,然后使用 Kibana(Elasticsearch/Logstash/Kibana 栈的一部分)来构建自定义仪表板以呈现对你很重要的可视化数据。此外，你可以使用 Elasticsearch 聚合功能来对数据执行复杂的商业智能查询。

在本教程的剩余部分中将指导你开始使用、运行、深入了解 Elasticsearch，以及执行如索引、搜索、和修改数据等基本操作。在本教程的最后，你应该会很好地了解Elasticsearch是什么，以及它是如何工作的，并希望从中可以收获启发以便利用它来构建复杂的搜索应用程序，或者从你的数据中挖掘情报。
--

== 基本概念

以下是一些关于 Elasticsearch 的核心概念。理解这些概念可以极大的简化学习过程。

[float]
=== 近实时 (NRT)

Elasticsearch 是一个近实时搜索平台。这意味着从你索引文档到其可搜索会有一个轻微的延迟（通常是 1s）。

[float]
=== 集群

集群是一个或多个节点（服务器）的集合，这些节点共同保存你的所有数据并提供联合索引和搜索能力。一个集群由一个唯一名称标识，默认名称是 "elasticsearch"。这个名称很重要，因为如果节点被设置为以名称连接集群，那么节点就只能是集群的一部分。

在不同的环境中请确保不要使用相同的集群名，否则可能导致节点加入错误的集群。例如，你可以使用 `logging-dev`、`logging-stage`、`logging-prod` 来标识开发、阶段、生产集群。

注意只有一个节点的集群是完全合适和合理的。此外，你也可以有多个唯一名称标识的独立集群。

[float]
=== 节点

节点是集群中的具有存储数据和参与集群索引、搜索功能的单台服务器。和集群类似，一个节点由一个名称标识，默认情况下该名称是在启动时分配的一个随机通用唯一识别码（UUID）。
如果你不想要默认名称则可以自定义。节点名称对于管理确认网络中哪些服务器在 Elasticsearch 集群中十分重要。

节点可以配置成加入按名称划分的集群。默认情况下，每个节点是设置成加入名为 `elasticsearch` 的集群，这意味着一旦你启动了网络中的多个节点，且假设这些节点可以相互发现，那么这些节点将自动组成一个名为 `elasticsearch` 的集群。

单个集群中可以有任意多的节点。此外，如果网络中没有其他正在运行的 Elasticsearch 节点，则默认情况下启动单个节点会形成一个名为 `elasticsearch` 的单节点集群。

[float]
=== 索引

索引是一个具有相似特性的文档集合。例如你可以使用三个索引分别存储客户数据、产品目录和订单数据。索引是由名称（必须全部为小写）标识的，同时该名称作为对文档进行索引、搜索、更新和删除操作的引用。

单个急群中你可以定义任意多的索引。

[float]
=== 类型

deprecated[6.0.0,参考 <<removal-of-types>>]

类型曾经是同一索引中允许存储不同类型文档的一个逻辑 类别/分区。例如一个是用户类型，另一个是博客文章类型。 现在索引中再也不能创建多个类型，同时在未来版本中将移除整个类型的概念。更多信息请参考 <<removal-of-types>>。

[float]
=== 文档

文档是可被索引的基本单元。例如你可以使用三个文档分布存储单个客户、单个产品以及单个订单。这个文档是由互联网中常见的数据交换格式 http://json.org/[JSON] (JavaScript Object Notation) 呈现的。

在索引/类型中可以存储任意多的文档。需要注意的是尽管文档表面上是存储在索引中，但实际上文档必须被 索引/分配 给索引内的类型。

[[getting-started-shards-and-replicas]]
[float]
=== 分片 & 副本

索引可以存储大量数据，这些数据可能会超出单个节点的硬件限制。例如，使用单个节点的磁盘存储占用 1TB 磁盘空间的十亿份文档不大合适，另外单节点在处理搜索请求也可能会比较慢。

为了解决这些问题，Elasticsearch 提供了将索引细分为多个分片的能力。当你创建索引时，你可以定义你需要的分片数，每个分片都是分布在集群中任意节点的一个功能齐全的独立 "index"。

分片如此重要主要有两个原因：

* 它允许水平 划分/缩放 你的内容。
* 它允许你跨分片（可能在多个节点上）进行分发和并行操作以达到提升 性能/吞吐量。


分片分发和文档搜索时如何聚合的机制是完全由 Elasticsearch 管理的，且对用户来说是透明的。

在 网络/云 环境下失败可能发生在任何时刻。因此强烈建议当发生 分片/节点 下线或消失时需要有一套故障转移机制。基于此，Elasticsearch 运行你设置一个或者多个索引分片的拷贝，这些副本被称为副本分片或简称副本。

复制如此重要主要有两个原因：

* 它在 分片/节点 失败的情况下提供了高可用。因为这个原因，需要特别注意的是副本分片用户不会分配在与 原始/主 分片相同的节点上。
* 它允许你扩展你的 搜索量/吞吐量，因为搜索可用在所有副本上执行。


总之，每个索引可以分为多个分片。一个索引可以有 0 个（意味着每一副本）或者多个副本。一旦设置了副本，每个索引都将有主分片（原始分片）和副分片（主分片的拷贝）。分片和副本数在创建索引时定义，创建索引之后，你可以随时动态改变副本数，但是不能随意改变分片数。

默认情况下，Elasticsearch 中的每个索引由 5 个主分片和 1 个副本组成，这意味着你的集群至少需要两个节点，索引将包含 5 个主分片和 5 个副本（1个完整副本），总共10个分片。

NOTE: 每个 Elasticsearch 分片是一个 Lucence 索引。一个 Lucene 索引有最大支持文档数。从 https://issues.apache.org/jira/browse/LUCENE-5843[`LUCENE-5843`] 开始, 最大限制是 `2,147,483,519` (= Integer.MAX_VALUE - 128)。
你可以使用 {ref}/cat-shards.html[`_cat/shards`] API 管理分片数。

有了这些，我们就可以开始有趣之旅了...

== 安装

Elasticsearch 最低需要Java 8。 在撰写本文时强烈建议你使用 Oracle JDK 版本 {jdk}。Java 在不同平台的安装方法不同，索引我们也不会在这里详细讨论安装细节。Oracle 的建议安装文档可以在 http://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html[Oracle's website]找到。
可以说，在安装 Elasticsearch 之前请先检查你的 Java 版本（请按需进行 安装/升级）：

[source,sh]
--------------------------------------------------
java -version
echo $JAVA_HOME
--------------------------------------------------

当设置好了 Java 环境之后，我们可以下载并允许 Elasticsearch。所有已发布的二进制版本可以从 http://www.elastic.co/downloads[`www.elastic.co/downloads`] 下载。对于每一个发布版本，你可以从 `zip` 、 `tar` 、 `DEB` 、 `RPM` 或者 Windows `MSI` 中选择合适版本进行安装。

[float]
=== 安装 tar 示例

简单起见，我们使用 {ref}/zip-targz.html[tar] 文件。

我们下载 Elasticsearc {version} tar：

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}.tar.gz
--------------------------------------------------
// NOTCONSOLE

然后进行解压：

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
tar -xvf elasticsearch-{version}.tar.gz
--------------------------------------------------

解压后当前目录中将创建一组文件和文件夹。接着进入 bin 目录:

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
cd elasticsearch-{version}/bin
--------------------------------------------------

下面我们准备开始我们的节点和单集群：

[source,sh]
--------------------------------------------------
./elasticsearch
--------------------------------------------------

[float]
=== 使用 Homebrew 安装

在 macOS, Elasticsearch 可以通过 https://brew.sh[Homebrew] 安装：

["source","sh"]
--------------------------------------------------
brew install elasticsearch
--------------------------------------------------

[float]
=== MSI Windows Installer 安装示例

对于 Windows 用户，我们推荐使用 {ref}/windows.html[MSI Installer package] 安装。 这个包包含图形用户界面（GUI）可以引导你完成安装。

首先，从 https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-{version}.msi 下载 Elasticsearch {version} MSI。

然后双击下载的文件以启动 GUI。在第一个界面中选择部署目录：

[[getting-started-msi-installer-locations]]
image::images/msi_installer/msi_installer_locations.png[]

选择是否安装为服务，或者按需手动启动 Elasticsearch。
为了与 tar 示例保持一致，选择不安装服务：

[[getting-started-msi-installer-service]]
image::images/msi_installer/msi_installer_no_service.png[]

只需保留配置的默认值：

[[getting-started-msi-installer-configuration]]
image::images/msi_installer/msi_installer_configuration.png[]

同样为了与 tar 示例保持一致，取消所有插件以避免安装任何插件:

[[getting-started-msi-installer-plugins]]
image::images/msi_installer/msi_installer_plugins.png[]

点击安装按钮后，Elasticsearch 将会开始安装：

[[getting-started-msi-installer-success]]
image::images/msi_installer/msi_installer_success.png[]

默认情况下 Elasticsearch 会安装在 `%PROGRAMFILES%\Elastic\Elasticsearch`。在这导航并进入 bin 目录：

**命令提示符：**

[source,sh]
--------------------------------------------------
cd %PROGRAMFILES%\Elastic\Elasticsearch\bin
--------------------------------------------------

**PowerShell：**

[source,powershell]
--------------------------------------------------
cd $env:PROGRAMFILES\Elastic\Elasticsearch\bin
--------------------------------------------------

现在启动节点和集群：

[source,sh]
--------------------------------------------------
.\elasticsearch.exe
--------------------------------------------------

[float]
=== 节点允许成功

如果一切顺利，你会看到如下的一串提示：

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
[2016-09-16T14:17:51,251][INFO ][o.e.n.Node               ] [] initializing ...
[2016-09-16T14:17:51,329][INFO ][o.e.e.NodeEnvironment    ] [6-bjhwl] using [1] data paths, mounts [[/ (/dev/sda1)]], net usable_space [317.7gb], net total_space [453.6gb], spins? [no], types [ext4]
[2016-09-16T14:17:51,330][INFO ][o.e.e.NodeEnvironment    ] [6-bjhwl] heap size [1.9gb], compressed ordinary object pointers [true]
[2016-09-16T14:17:51,333][INFO ][o.e.n.Node               ] [6-bjhwl] node name [6-bjhwl] derived from node ID; set [node.name] to override
[2016-09-16T14:17:51,334][INFO ][o.e.n.Node               ] [6-bjhwl] version[{version}], pid[21261], build[f5daa16/2016-09-16T09:12:24.346Z], OS[Linux/4.4.0-36-generic/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_60/25.60-b23]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [aggs-matrix-stats]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [ingest-common]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [lang-expression]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [lang-mustache]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [lang-painless]
[2016-09-16T14:17:51,967][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [percolator]
[2016-09-16T14:17:51,968][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [reindex]
[2016-09-16T14:17:51,968][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [transport-netty3]
[2016-09-16T14:17:51,968][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded module [transport-netty4]
[2016-09-16T14:17:51,968][INFO ][o.e.p.PluginsService     ] [6-bjhwl] loaded plugin [mapper-murmur3]
[2016-09-16T14:17:53,521][INFO ][o.e.n.Node               ] [6-bjhwl] initialized
[2016-09-16T14:17:53,521][INFO ][o.e.n.Node               ] [6-bjhwl] starting ...
[2016-09-16T14:17:53,671][INFO ][o.e.t.TransportService   ] [6-bjhwl] publish_address {192.168.8.112:9300}, bound_addresses {{192.168.8.112:9300}
[2016-09-16T14:17:53,676][WARN ][o.e.b.BootstrapCheck     ] [6-bjhwl] max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]
[2016-09-16T14:17:56,718][INFO ][o.e.c.s.ClusterService   ] [6-bjhwl] new_master {6-bjhwl}{6-bjhwl4TkajjoD2oEipnQ}{8m3SNKoFR6yQl1I0JUfPig}{192.168.8.112}{192.168.8.112:9300}, reason: zen-disco-elected-as-master ([0] nodes joined)
[2016-09-16T14:17:56,731][INFO ][o.e.h.HttpServer         ] [6-bjhwl] publish_address {192.168.8.112:9200}, bound_addresses {[::1]:9200}, {192.168.8.112:9200}
[2016-09-16T14:17:56,732][INFO ][o.e.g.GatewayService     ] [6-bjhwl] recovered [0] indices into cluster_state
[2016-09-16T14:17:56,748][INFO ][o.e.n.Node               ] [6-bjhwl] started
--------------------------------------------------

在忽略细节的前提下，我们可以看到我们的节点名为 "6-bjhwl"（在你的场景下会看到不同的字符标识）在单集群中已经被选为主节点。 现在不用太关心主节点，最重要的是我们已经在急群中启动了一个节点。

如上所述，我们可以重命名集群或节点名。这可以在启动 Elasticsearch 时使用命令行：

[source,sh]
--------------------------------------------------
./elasticsearch -Ecluster.name=my_cluster_name -Enode.name=my_node_name
--------------------------------------------------

还需注意的是由 HTTP 地址（`192.168.8.112`）和端口（`9200`）标记的节点是可访问的。默认情况下，Elasticsearch 使用端口 `9200` 来提供 REST API 访问。如果需要，这个端口是可配置的。

== 探索你的集群

[float]
=== The REST API

Now that we have our node (and cluster) up and running, the next step is to understand how to communicate with it. Fortunately, Elasticsearch provides a very comprehensive and powerful REST API that you can use to interact with your cluster. Among the few things that can be done with the API are as follows:

* Check your cluster, node, and index health, status, and statistics
* Administer your cluster, node, and index data and metadata
* Perform CRUD (Create, Read, Update, and Delete) and search operations against your indexes
* Execute advanced search operations such as paging, sorting, filtering, scripting, aggregations, and many others

=== Cluster Health

Let's start with a basic health check, which we can use to see how our cluster is doing. We'll be using curl to do this but you can use any tool that allows you to make HTTP/REST calls. Let's assume that we are still on the same node where we started Elasticsearch on and open another command shell window.

To check the cluster health, we will be using the {ref}/cat.html[`_cat` API]. You can
run the command below in {kibana-ref}/console-kibana.html[Kibana's Console]
by clicking "VIEW IN CONSOLE" or with `curl` by clicking the "COPY AS CURL"
link below and pasting it into a terminal.

[source,js]
--------------------------------------------------
GET /_cat/health?v
--------------------------------------------------
// CONSOLE

And the response:

[source,txt]
--------------------------------------------------
epoch      timestamp cluster       status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1475247709 17:01:49  elasticsearch green           1         1      0   0    0    0        0             0                  -                100.0%
--------------------------------------------------
// TESTRESPONSE[s/1475247709 17:01:49  elasticsearch/\\d+ \\d+:\\d+:\\d+ docs_integTestCluster/]
// TESTRESPONSE[s/0             0                  -/0             \\d+                  -/]
// TESTRESPONSE[_cat]

We can see that our cluster named "elasticsearch" is up with a green status.

Whenever we ask for the cluster health, we either get green, yellow, or red.

    * Green - everything is good (cluster is fully functional)
    * Yellow - all data is available but some replicas are not yet allocated (cluster is fully functional)
    * Red - some data is not available for whatever reason (cluster is partially functional)

**Note:** When a cluster is red, it will continue to serve search requests from the available shards but you will likely need to fix it ASAP since there are unassigned shards.

Also from the above response, we can see a total of 1 node and that we have 0 shards since we have no data in it yet. Note that since we are using the default cluster name (elasticsearch) and since Elasticsearch uses unicast network discovery by default to find other nodes on the same machine, it is possible that you could accidentally start up more than one node on your computer and have them all join a single cluster. In this scenario, you may see more than 1 node in the above response.

We can also get a list of nodes in our cluster as follows:

[source,js]
--------------------------------------------------
GET /_cat/nodes?v
--------------------------------------------------
// CONSOLE

And the response:

[source,txt]
--------------------------------------------------
ip        heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
127.0.0.1           10           5   5    4.46                        mdi      *      PB2SGZY
--------------------------------------------------
// TESTRESPONSE[s/10           5   5    4.46/\\d+ \\d+ \\d+ (\\d+\\.\\d+)? (\\d+\\.\\d+)? (\\d+\.\\d+)?/]
// TESTRESPONSE[s/[*]/[*]/ s/PB2SGZY/.+/ _cat]

Here, we can see our one node named "PB2SGZY", which is the single node that is currently in our cluster.

=== List All Indices

Now let's take a peek at our indices:

[source,js]
--------------------------------------------------
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE

And the response:

[source,txt]
--------------------------------------------------
health status index uuid pri rep docs.count docs.deleted store.size pri.store.size
--------------------------------------------------
// TESTRESPONSE[_cat]

Which simply means we have no indices yet in the cluster.

=== Create an Index

Now let's create an index named "customer" and then list all the indexes again:

[source,js]
--------------------------------------------------
PUT /customer?pretty
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE

The first command creates the index named "customer" using the PUT verb. We simply append `pretty` to the end of the call to tell it to pretty-print the JSON response (if any).

And the response:

[source,txt]
--------------------------------------------------
health status index    uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   customer 95SQ4TSUT7mWBT7VNHH67A   5   1          0            0       260b           260b
--------------------------------------------------
// TESTRESPONSE[s/95SQ4TSUT7mWBT7VNHH67A/.+/ s/260b/\\d+\\.?\\d?k?b/ _cat]

The results of the second command tells us that we now have 1 index named customer and it has 5 primary shards and 1 replica (the defaults) and it contains 0 documents in it.

You might also notice that the customer index has a yellow health tagged to it. Recall from our previous discussion that yellow means that some replicas are not (yet) allocated. The reason this happens for this index is because Elasticsearch by default created one replica for this index. Since we only have one node running at the moment, that one replica cannot yet be allocated (for high availability) until a later point in time when another node joins the cluster. Once that replica gets allocated onto a second node, the health status for this index will turn to green.

=== Index and Query a Document

Let's now put something into our customer index. We'll index a simple customer document into the customer index, with an ID of 1 as follows:

[source,js]
--------------------------------------------------
PUT /customer/doc/1?pretty
{
  "name": "John Doe"
}
--------------------------------------------------
// CONSOLE

And the response:

[source,js]
--------------------------------------------------
{
  "_index" : "customer",
  "_type" : "doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
--------------------------------------------------
// TESTRESPONSE[s/"_seq_no" : 0/"_seq_no" : $body._seq_no/ s/"_primary_term" : 1/"_primary_term" : $body._primary_term/]

From the above, we can see that a new customer document was successfully created inside the customer index. The document also has an internal id of 1 which we specified at index time.

It is important to note that Elasticsearch does not require you to explicitly create an index first before you can index documents into it. In the previous example, Elasticsearch will automatically create the customer index if it didn't already exist beforehand.

Let's now retrieve that document that we just indexed:

[source,js]
--------------------------------------------------
GET /customer/doc/1?pretty
--------------------------------------------------
// CONSOLE
// TEST[continued]

And the response:

[source,js]
--------------------------------------------------
{
  "_index" : "customer",
  "_type" : "doc",
  "_id" : "1",
  "_version" : 1,
  "found" : true,
  "_source" : { "name": "John Doe" }
}
--------------------------------------------------
// TESTRESPONSE

Nothing out of the ordinary here other than a field, `found`, stating that we found a document with the requested ID 1 and another field, `_source`, which returns the full JSON document that we indexed from the previous step.

=== Delete an Index

Now let's delete the index that we just created and then list all the indexes again:

[source,js]
--------------------------------------------------
DELETE /customer?pretty
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE
// TEST[continued]

And the response:

[source,txt]
--------------------------------------------------
health status index uuid pri rep docs.count docs.deleted store.size pri.store.size
--------------------------------------------------
// TESTRESPONSE[_cat]

Which means that the index was deleted successfully and we are now back to where we started with nothing in our cluster.

Before we move on, let's take a closer look again at some of the API commands that we have learned so far:

[source,js]
--------------------------------------------------
PUT /customer
PUT /customer/doc/1
{
  "name": "John Doe"
}
GET /customer/doc/1
DELETE /customer
--------------------------------------------------
// CONSOLE

If we study the above commands carefully, we can actually see a pattern of how we access data in Elasticsearch. That pattern can be summarized as follows:

[source,js]
--------------------------------------------------
<REST Verb> /<Index>/<Type>/<ID>
--------------------------------------------------
// NOTCONSOLE

This REST access pattern is so pervasive throughout all the API commands that if you can simply remember it, you will have a good head start at mastering Elasticsearch.

== Modifying Your Data

Elasticsearch provides data manipulation and search capabilities in near real time. By default, you can expect a one second delay (refresh interval) from the time you index/update/delete your data until the time that it appears in your search results. This is an important distinction from other platforms like SQL wherein data is immediately available after a transaction is completed.

[float]
=== Indexing/Replacing Documents

We've previously seen how we can index a single document. Let's recall that command again:

[source,js]
--------------------------------------------------
PUT /customer/doc/1?pretty
{
  "name": "John Doe"
}
--------------------------------------------------
// CONSOLE

Again, the above will index the specified document into the customer index, with the ID of 1. If we then executed the above command again with a different (or same) document, Elasticsearch will replace (i.e. reindex) a new document on top of the existing one with the ID of 1:

[source,js]
--------------------------------------------------
PUT /customer/doc/1?pretty
{
  "name": "Jane Doe"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

The above changes the name of the document with the ID of 1 from "John Doe" to "Jane Doe". If, on the other hand, we use a different ID, a new document will be indexed and the existing document(s) already in the index remains untouched.

[source,js]
--------------------------------------------------
PUT /customer/doc/2?pretty
{
  "name": "Jane Doe"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

The above indexes a new document with an ID of 2.

When indexing, the ID part is optional. If not specified, Elasticsearch will generate a random ID and then use it to index the document. The actual ID Elasticsearch generates (or whatever we specified explicitly in the previous examples) is returned as part of the index API call.

This example shows how to index a document without an explicit ID:

[source,js]
--------------------------------------------------
POST /customer/doc?pretty
{
  "name": "Jane Doe"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Note that in the above case, we are using the `POST` verb instead of PUT since we didn't specify an ID.

=== Updating Documents

In addition to being able to index and replace documents, we can also update documents. Note though that Elasticsearch does not actually do in-place updates under the hood. Whenever we do an update, Elasticsearch deletes the old document and then indexes a new document with the update applied to it in one shot.

This example shows how to update our previous document (ID of 1) by changing the name field to "Jane Doe":

[source,js]
--------------------------------------------------
POST /customer/doc/1/_update?pretty
{
  "doc": { "name": "Jane Doe" }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example shows how to update our previous document (ID of 1) by changing the name field to "Jane Doe" and at the same time add an age field to it:

[source,js]
--------------------------------------------------
POST /customer/doc/1/_update?pretty
{
  "doc": { "name": "Jane Doe", "age": 20 }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Updates can also be performed by using simple scripts. This example uses a script to increment the age by 5:

[source,js]
--------------------------------------------------
POST /customer/doc/1/_update?pretty
{
  "script" : "ctx._source.age += 5"
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In the above example, `ctx._source` refers to the current source document that is about to be updated.

Elasticsearch provides the ability to update multiple documents given a query condition (like an `SQL UPDATE-WHERE` statement). See {ref}/docs-update-by-query.html[`docs-update-by-query` API]

=== Deleting Documents

Deleting a document is fairly straightforward. This example shows how to delete our previous customer with the ID of 2:

[source,js]
--------------------------------------------------
DELETE /customer/doc/2?pretty
--------------------------------------------------
// CONSOLE
// TEST[continued]

See the {ref}/docs-delete-by-query.html[`_delete_by_query` API] to delete all documents matching a specific query.
It is worth noting that it is much more efficient to delete a whole index
instead of deleting all documents with the Delete By Query API.

=== Batch Processing

In addition to being able to index, update, and delete individual documents, Elasticsearch also provides the ability to perform any of the above operations in batches using the {ref}/docs-bulk.html[`_bulk` API]. This functionality is important in that it provides a very efficient mechanism to do multiple operations as fast as possible with as few network roundtrips as possible.

As a quick example, the following call indexes two documents (ID 1 - John Doe and ID 2 - Jane Doe) in one bulk operation:

[source,js]
--------------------------------------------------
POST /customer/doc/_bulk?pretty
{"index":{"_id":"1"}}
{"name": "John Doe" }
{"index":{"_id":"2"}}
{"name": "Jane Doe" }
--------------------------------------------------
// CONSOLE

This example updates the first document (ID of 1) and then deletes the second document (ID of 2) in one bulk operation:

[source,sh]
--------------------------------------------------
POST /customer/doc/_bulk?pretty
{"update":{"_id":"1"}}
{"doc": { "name": "John Doe becomes Jane Doe" } }
{"delete":{"_id":"2"}}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Note above that for the delete action, there is no corresponding source document after it since deletes only require the ID of the document to be deleted.

The Bulk API does not fail due to failures in one of the actions. If a single action fails for whatever reason, it will continue to process the remainder of the actions after it. When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not.

== Exploring Your Data

[float]
=== Sample Dataset

Now that we've gotten a glimpse of the basics, let's try to work on a more realistic dataset. I've prepared a sample of fictitious JSON documents of customer bank account information. Each document has the following schema:

[source,js]
--------------------------------------------------
{
    "account_number": 0,
    "balance": 16623,
    "firstname": "Bradshaw",
    "lastname": "Mckenzie",
    "age": 29,
    "gender": "F",
    "address": "244 Columbus Place",
    "employer": "Euron",
    "email": "bradshawmckenzie@euron.com",
    "city": "Hobucken",
    "state": "CO"
}
--------------------------------------------------
// NOTCONSOLE

For the curious, this data was generated using http://www.json-generator.com/[`www.json-generator.com/`], so please ignore the actual values and semantics of the data as these are all randomly generated.

[float]
=== Loading the Sample Dataset

You can download the sample dataset (accounts.json) from https://github.com/elastic/elasticsearch/blob/master/docs/src/test/resources/accounts.json?raw=true[here]. Extract it to our current directory and let's load it into our cluster as follows:

[source,sh]
--------------------------------------------------
curl -H "Content-Type: application/json" -XPOST 'localhost:9200/bank/account/_bulk?pretty&refresh' --data-binary "@accounts.json"
curl 'localhost:9200/_cat/indices?v'
--------------------------------------------------
// NOTCONSOLE

////
This replicates the above in a document-testing friendly way but isn't visible
in the docs:

[source,js]
--------------------------------------------------
GET /_cat/indices?v
--------------------------------------------------
// CONSOLE
// TEST[setup:bank]
////

And the response:

[source,txt]
--------------------------------------------------
health status index uuid                   pri rep docs.count docs.deleted store.size pri.store.size
yellow open   bank  l7sSYV2cQXmu6_4rJWVIww   5   1       1000            0    128.6kb        128.6kb
--------------------------------------------------
// TESTRESPONSE[s/128.6kb/\\d+(\\.\\d+)?[mk]?b/]
// TESTRESPONSE[s/l7sSYV2cQXmu6_4rJWVIww/.+/ _cat]

Which means that we just successfully bulk indexed 1000 documents into the bank index (under the account type).

=== The Search API

Now let's start with some simple searches. There are two basic ways to run searches: one is by sending search parameters through the {ref}/search-uri-request.html[REST request URI] and the other by sending them through the {ref}/search-request-body.html[REST request body]. The request body method allows you to be more expressive and also to define your searches in a more readable JSON format. We'll try one example of the request URI method but for the remainder of this tutorial, we will exclusively be using the request body method.

The REST API for search is accessible from the `_search` endpoint. This example returns all documents in the bank index:

[source,js]
--------------------------------------------------
GET /bank/_search?q=*&sort=account_number:asc&pretty
--------------------------------------------------
// CONSOLE
// TEST[continued]

Let's first dissect the search call. We are searching (`_search` endpoint) in the bank index, and the `q=*` parameter instructs Elasticsearch to match all documents in the index. The `sort=account_number:asc` parameter indicates to sort the results using the `account_number` field of each document in an ascending order. The `pretty` parameter, again, just tells Elasticsearch to return pretty-printed JSON results.

And the response (partially shown):

[source,js]
--------------------------------------------------
{
  "took" : 63,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1000,
    "max_score" : null,
    "hits" : [ {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "0",
      "sort": [0],
      "_score" : null,
      "_source" : {"account_number":0,"balance":16623,"firstname":"Bradshaw","lastname":"Mckenzie","age":29,"gender":"F","address":"244 Columbus Place","employer":"Euron","email":"bradshawmckenzie@euron.com","city":"Hobucken","state":"CO"}
    }, {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "1",
      "sort": [1],
      "_score" : null,
      "_source" : {"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"}
    }, ...
    ]
  }
}
--------------------------------------------------
// TESTRESPONSE[s/"took" : 63/"took" : $body.took/]
// TESTRESPONSE[s/\.\.\./$body.hits.hits.2, $body.hits.hits.3, $body.hits.hits.4, $body.hits.hits.5, $body.hits.hits.6, $body.hits.hits.7, $body.hits.hits.8, $body.hits.hits.9/]

As for the response, we see the following parts:

* `took` – time in milliseconds for Elasticsearch to execute the search
* `timed_out` – tells us if the search timed out or not
* `_shards` – tells us how many shards were searched, as well as a count of the successful/failed searched shards
* `hits` – search results
* `hits.total` – total number of documents matching our search criteria
* `hits.hits` – actual array of search results (defaults to first 10 documents)
* `hits.sort` - sort key for results (missing if sorting by score)
* `hits._score` and `max_score` - ignore these fields for now

Here is the same exact search above using the alternative request body method:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "sort": [
    { "account_number": "asc" }
  ]
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

The difference here is that instead of passing `q=*` in the URI, we POST a JSON-style query request body to the `_search` API. We'll discuss this JSON query in the next section.

////
Hidden response just so we can assert that it is indeed the same but don't have
to clutter the docs with it:

[source,js]
--------------------------------------------------
{
  "took" : 63,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 1000,
    "max_score": null,
    "hits" : [ {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "0",
      "sort": [0],
      "_score": null,
      "_source" : {"account_number":0,"balance":16623,"firstname":"Bradshaw","lastname":"Mckenzie","age":29,"gender":"F","address":"244 Columbus Place","employer":"Euron","email":"bradshawmckenzie@euron.com","city":"Hobucken","state":"CO"}
    }, {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "1",
      "sort": [1],
      "_score": null,
      "_source" : {"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"}
    }, ...
    ]
  }
}
--------------------------------------------------
// TESTRESPONSE[s/"took" : 63/"took" : $body.took/]
// TESTRESPONSE[s/\.\.\./$body.hits.hits.2, $body.hits.hits.3, $body.hits.hits.4, $body.hits.hits.5, $body.hits.hits.6, $body.hits.hits.7, $body.hits.hits.8, $body.hits.hits.9/]

////

It is important to understand that once you get your search results back, Elasticsearch is completely done with the request and does not maintain any kind of server-side resources or open cursors into your results. This is in stark contrast to many other platforms such as SQL wherein you may initially get a partial subset of your query results up-front and then you have to continuously go back to the server if you want to fetch (or page through) the rest of the results using some kind of stateful server-side cursor.

=== Introducing the Query Language

Elasticsearch provides a JSON-style domain-specific language that you can use to execute queries. This is referred to as the {ref}/query-dsl.html[Query DSL]. The query language is quite comprehensive and can be intimidating at first glance but the best way to actually learn it is to start with a few basic examples.

Going back to our last example, we executed this query:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Dissecting the above, the `query` part tells us what our query definition is and the `match_all` part is simply the type of query that we want to run. The `match_all` query is simply a search for all documents in the specified index.

In addition to the `query` parameter, we also can pass other parameters to
influence the search results. In the example in the section above we passed in
`sort`, here we pass in `size`:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "size": 1
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Note that if `size` is not specified, it defaults to 10.

This example does a `match_all` and returns documents 11 through 20:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "from": 10,
  "size": 10
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

The `from` parameter (0-based) specifies which document index to start from and the `size` parameter specifies how many documents to return starting at the from parameter. This feature is useful when implementing paging of search results. Note that if `from` is not specified, it defaults to 0.

This example does a `match_all` and sorts the results by account balance in descending order and returns the top 10 (default size) documents.

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "sort": { "balance": { "order": "desc" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

=== Executing Searches

Now that we have seen a few of the basic search parameters, let's dig in some more into the Query DSL. Let's first take a look at the returned document fields. By default, the full JSON document is returned as part of all searches. This is referred to as the source (`_source` field in the search hits). If we don't want the entire source document returned, we have the ability to request only a few fields from within source to be returned.

This example shows how to return two fields, `account_number` and `balance` (inside of `_source`), from the search:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_all": {} },
  "_source": ["account_number", "balance"]
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Note that the above example simply reduces the `_source` field. It will still only return one field named `_source` but within it, only the fields `account_number` and `balance` are included.

If you come from a SQL background, the above is somewhat similar in concept to the `SQL SELECT FROM` field list.

Now let's move on to the query part. Previously, we've seen how the `match_all` query is used to match all documents. Let's now introduce a new query called the {ref}/query-dsl-match-query.html[`match` query], which can be thought of as a basic fielded search query (i.e. a search done against a specific field or set of fields).

This example returns the account numbered 20:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "account_number": 20 } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example returns all accounts containing the term "mill" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "address": "mill" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example returns all accounts containing the term "mill" or "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match": { "address": "mill lane" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example is a variant of `match` (`match_phrase`) that returns all accounts containing the phrase "mill lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": { "match_phrase": { "address": "mill lane" } }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Let's now introduce the {ref}/query-dsl-bool-query.html[`bool` query]. The `bool` query allows us to compose smaller queries into bigger queries using boolean logic.

This example composes two `match` queries and returns all accounts containing "mill" and "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In the above example, the `bool must` clause specifies all the queries that must be true for a document to be considered a match.

In contrast, this example composes two `match` queries and returns all accounts containing "mill" or "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In the above example, the `bool should` clause specifies a list of queries either of which must be true for a document to be considered a match.

This example composes two `match` queries and returns all accounts that contain neither "mill" nor "lane" in the address:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must_not": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In the above example, the `bool must_not` clause specifies a list of queries none of which must be true for a document to be considered a match.

We can combine `must`, `should`, and `must_not` clauses simultaneously inside a `bool` query. Furthermore, we can compose `bool` queries inside any of these `bool` clauses to mimic any complex multi-level boolean logic.

This example returns all accounts of anybody who is 40 years old but doesn't live in ID(aho):

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "age": "40" } }
      ],
      "must_not": [
        { "match": { "state": "ID" } }
      ]
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

=== Executing Filters

In the previous section, we skipped over a little detail called the document score (`_score` field in the search results). The score is a numeric value that is a relative measure of how well the document matches the search query that we specified. The higher the score, the more relevant the document is, the lower the score, the less relevant the document is.

But queries do not always need to produce scores, in particular when they are only used for "filtering" the document set. Elasticsearch detects these situations and automatically optimizes query execution in order not to compute useless scores.

The {ref}/query-dsl-bool-query.html[`bool` query] that we introduced in the previous section also supports `filter` clauses which allow to use a query to restrict the documents that will be matched by other clauses, without changing how scores are computed. As an example, let's introduce the {ref}/query-dsl-range-query.html[`range` query], which allows us to filter documents by a range of values. This is generally used for numeric or date filtering.

This example uses a bool query to return all accounts with balances between 20000 and 30000, inclusive. In other words, we want to find accounts with a balance that is greater than or equal to 20000 and less than or equal to 30000.

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "query": {
    "bool": {
      "must": { "match_all": {} },
      "filter": {
        "range": {
          "balance": {
            "gte": 20000,
            "lte": 30000
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Dissecting the above, the bool query contains a `match_all` query (the query part) and a `range` query (the filter part). We can substitute any other queries into the query and the filter parts. In the above case, the range query makes perfect sense since documents falling into the range all match "equally", i.e., no document is more relevant than another.

In addition to the `match_all`, `match`, `bool`, and `range` queries, there are a lot of other query types that are available and we won't go into them here. Since we already have a basic understanding of how they work, it shouldn't be too difficult to apply this knowledge in learning and experimenting with the other query types.

=== Executing Aggregations

Aggregations provide the ability to group and extract statistics from your data. The easiest way to think about aggregations is by roughly equating it to the SQL GROUP BY and the SQL aggregate functions. In Elasticsearch, you have the ability to execute searches returning hits and at the same time return aggregated results separate from the hits all in one response. This is very powerful and efficient in the sense that you can run queries and multiple aggregations and get the results back of both (or either) operations in one shot avoiding network roundtrips using a concise and simplified API.

To start with, this example groups all the accounts by state, and then returns the top 10 (default) states sorted by count descending (also default):

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

In SQL, the above aggregation is similar in concept to:

[source,sh]
--------------------------------------------------
SELECT state, COUNT(*) FROM bank GROUP BY state ORDER BY COUNT(*) DESC
--------------------------------------------------

And the response (partially shown):

[source,js]
--------------------------------------------------
{
  "took": 29,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped" : 0,
    "failed": 0
  },
  "hits" : {
    "total" : 1000,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "group_by_state" : {
      "doc_count_error_upper_bound": 20,
      "sum_other_doc_count": 770,
      "buckets" : [ {
        "key" : "ID",
        "doc_count" : 27
      }, {
        "key" : "TX",
        "doc_count" : 27
      }, {
        "key" : "AL",
        "doc_count" : 25
      }, {
        "key" : "MD",
        "doc_count" : 25
      }, {
        "key" : "TN",
        "doc_count" : 23
      }, {
        "key" : "MA",
        "doc_count" : 21
      }, {
        "key" : "NC",
        "doc_count" : 21
      }, {
        "key" : "ND",
        "doc_count" : 21
      }, {
        "key" : "ME",
        "doc_count" : 20
      }, {
        "key" : "MO",
        "doc_count" : 20
      } ]
    }
  }
}
--------------------------------------------------
// TESTRESPONSE[s/"took": 29/"took": $body.took/]

We can see that there are 27 accounts in `ID` (Idaho), followed by 27 accounts
in `TX` (Texas), followed by 25 accounts in `AL` (Alabama), and so forth.

Note that we set `size=0` to not show search hits because we only want to see the aggregation results in the response.

Building on the previous aggregation, this example calculates the average account balance by state (again only for the top 10 states sorted by count in descending order):

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

Notice how we nested the `average_balance` aggregation inside the `group_by_state` aggregation. This is a common pattern for all the aggregations. You can nest aggregations inside aggregations arbitrarily to extract pivoted summarizations that you require from your data.

Building on the previous aggregation, let's now sort on the average balance in descending order:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword",
        "order": {
          "average_balance": "desc"
        }
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

This example demonstrates how we can group by age brackets (ages 20-29, 30-39, and 40-49), then by gender, and then finally get the average account balance, per age bracket, per gender:

[source,js]
--------------------------------------------------
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_age": {
      "range": {
        "field": "age",
        "ranges": [
          {
            "from": 20,
            "to": 30
          },
          {
            "from": 30,
            "to": 40
          },
          {
            "from": 40,
            "to": 50
          }
        ]
      },
      "aggs": {
        "group_by_gender": {
          "terms": {
            "field": "gender.keyword"
          },
          "aggs": {
            "average_balance": {
              "avg": {
                "field": "balance"
              }
            }
          }
        }
      }
    }
  }
}
--------------------------------------------------
// CONSOLE
// TEST[continued]

There are many other aggregations capabilities that we won't go into detail here. The {ref}/search-aggregations.html[aggregations reference guide] is a great starting point if you want to do further experimentation.

== Conclusion

Elasticsearch is both a simple and complex product. We've so far learned the basics of what it is, how to look inside of it, and how to work with it using some of the REST APIs. Hopefully this tutorial has given you a better understanding of what Elasticsearch is and more importantly, inspired you to further experiment with the rest of its great features!
